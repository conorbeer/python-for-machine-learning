{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Regression\n",
    "\n",
    "In this lesson, you will be introduced to regression as one kind of supervised learning method.\n",
    "\n",
    "By the end of this lesson, you will be able to describe:\n",
    "- Regression.\n",
    "- Properties of linear regression.\n",
    "- Modern real-life applications of regression.\n",
    "- Regression metrics.\n",
    "\n",
    "In the previous lesson, you learned classification methods.\n",
    "\n",
    "**Housing Prices** \n",
    "\n",
    "Assume you want to buy a house. You go to your real estate agent and tell them that you want to buy a property. The real estate agent starts by asking you questions about your neighborhood, community, and preferences, including: the size of the house, the number of bedrooms, bathrooms, and any special features you require.\n",
    "\n",
    "The real estate agent then locates a number of suitable properties based on these features and helps you through the home buying process.\n",
    "\n",
    "Regression basically does the same thing. Regression is a statistical method that aims at finding relationships between different variables that are generalized enough to determine one variable using the others. Based on previous knowledge of similar data samples, machine learning models can fit a curve through regression to map the input variables of continuous numeric results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "### Linear Regression in Python\n",
    "In the following section, you will learn how to write Python code using the Scikit-learn implementation of a linear regression algorithm.\n",
    "In this section, you do not need your computer. You will simply read and follow the example, unless you want to run the code step-by-step.\n",
    "\n",
    "First, read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd \n",
    "df = pd.read_csv(“data.csv”)\n",
    "\n",
    "```\n",
    "\n",
    "Normalize your data using the Scikit-learn standard scaler.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n",
    "\n",
    "```\n",
    "\n",
    "Divide your data into train and test sets.\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    " test_size=0.33, random_state=0)\n",
    "\n",
    " ```\n",
    "\n",
    "Use the Scikit-learn implementation of linear regression.\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression()\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Use the instance of LinearRegression() you created to predict your data.\n",
    "\n",
    "```\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "```\n",
    "Collect the R-squared value after fitting.\n",
    "Hint:\n",
    "The function LinearRegression().score returns the r2_score of the prediction.\n",
    "\n",
    "```\n",
    "\n",
    "score = model.score(X_test,y_test)\n",
    "print(score)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission: Estimating Housing Prices Using Linear Regression\n",
    "\n",
    "In this mission, you will work with the Boston Housing dataset. You will practice how to extract and perform regression modeling on the Boston Housing dataset using linear regression. You will then evaluate the developer regressor.\n",
    "\n",
    "Instructions\n",
    "Perform the following task to complete this mission and write the codes in the provided editor:\n",
    "\n",
    "Try regression with the Boston Housing dataset using linear regression!\n",
    "\n",
    "Your code should perform the following tasks to complete this mission:\n",
    "\n",
    "Train a LinearRegression model using X_train and y_train.\n",
    "\n",
    "Score your model using X_test and y_test.\n",
    "\n",
    "Your code should return the score of the model using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6705795412578821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission: Regression with Iris\n",
    "\n",
    "In this mission, you will work with the Iris dataset. You will practice how to extract and perform regression modeling on the Iris dataset using linear regression. You will then evaluate the developer regressor.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Perform the following task to complete this mission and write the code in the provided editor:\n",
    "\n",
    "Try regression with the Iris dataset using linear regression!\n",
    "\n",
    "Your code should perform the following tasks to complete this mission:\n",
    "\n",
    "Train a LinearRegression model using sepal_train and petal_length_train.\n",
    "\n",
    "Score your model using sepal_test and petal_length_test.\n",
    "\n",
    "Your code should return the score of the model using the test dataset.\n",
    "\n",
    "Revisit the Iris dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    from sklearn.datasets import load_iris\n",
    "    X, _ = load_iris(return_X_y=True )\n",
    "\n",
    "    sepal = X[:, :2]\n",
    "    petal_length = X[:, 2]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    sepal_train, sepal_test, petal_length_train, petal_length_test = train_test_split(sepal, petal_length, test_size=0.33, random_state=0)\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(sepal_train, petal_length_train)\n",
    "\n",
    "    petal_length_pred = model.predict(sepal_test)\n",
    "\n",
    "    score = model.score(sepal_test, petal_length_test)\n",
    "\n",
    "    print(score)\n",
    "\n",
    "    return(score)\n",
    "\n",
    "main()\n",
    "\n",
    "# return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "Support vector machine regression simply follows the same procedure and rules as the support vector machine classification process with only a few minor differences. Most notably, the algorithm aims at satisfying the maximal possible margin and the least regression error.\n",
    "Since the output in the case of SVM regression is a real, continuous output, you need an approximation to be able to predict a definite continuous value given the data points.\n",
    "Consider the example shown below. The SVM aims at finding the hyperplane that decreases the least square errors to the minimum.\n",
    "In SVM for regression problem, you need the data point to be as close as possible to the chosen hyperplane. The SVM regression inherited this difference from simple regression (like ordinary least square) so that you can define a range from both sides of the hyperplane to make the regression function insensitive to the error.\n",
    "Eventually, SVM in regression has a boundary, like SVM in classification. However, the boundary for regression is for making the regression function insensitive with respect to the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM\n",
    "\n",
    "Kernel functions are functions that represent the data using a higher dimensional space to make it possible to fit with a linear hyperplane in the higher dimension.\n",
    "The difference between nonlinear and linear regression can be explained in the image below:\n",
    "\n",
    "The linear model will always fit to any data in a linear way, which in this case is insufficient and unrealistic. Therefore, higher orders are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement SVM Regression\n",
    "\n",
    "You are going to use an SVM to train your classifier. Because, you are going to perform a classification task, you will use the support vector classifier class.\n",
    "You will use the Boston Housing dataset, so you will import the datasets from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data, and split it into training and testing data using the sklearn.modelselection.train_test_split() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test =  model_selection.train_test_split(X, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\cbeer\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the most important step.\n",
    "Your data for this example is simple and linearly separable, so you will use the linear kernel.\n",
    "Also, if you cannot visualize your data (more than three dimensions) always try the linear kernel first.\n",
    "Arguments:\n",
    "1. Kernel: the kernel used to implement the kernel trick, as was previously discussed. The kernel argument can take multiple values, such as:\n",
    "Linear: for a simple linear equation kernel.\n",
    "Poly/RBF: polynomial and radial basis function equations, which are useful for creation of nonlinear hyperplanes.\n",
    "2. Gamma: gamma is mainly used with nonlinear hyperplanes. It represents how hard or soft the SVM margin would be. The higher the gamma, the greater accuracy the SVM is trying to achieve for lowest misclassification error and highest separation margin. It can be tuned to avoid overfitting.\n",
    "3. C: penalty parameter C of the error term. Could also be tuned to avoid overfitting of the SVM by creating a hard margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7120644131806819\n"
     ]
    }
   ],
   "source": [
    "reg = SVR(kernel='linear')\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(reg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 79.60870712971428\n",
      "R-squared: 0.01257013727196088\n"
     ]
    }
   ],
   "source": [
    "### Estimating house prices using SVM\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "model = SVR(C= 1.0, kernel = \"rbf\", gamma = 'auto')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('MSE: ' + str(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "print(\"R-squared: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.45971785004541466\n"
     ]
    }
   ],
   "source": [
    "### Iris regression SVM\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X, _ = load_iris(return_X_y=True)\n",
    "\n",
    "sepal = X[:, :2]\n",
    "petal_length = X[:, 2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sepal_train, sepal_test, petal_length_train, petal_length_test = train_test_split(sepal, petal_length, test_size=0.33, random_state=0)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR(kernel = 'linear', C = 0.2, gamma = 'auto', epsilon = 0.1)\n",
    "\n",
    "model.fit(sepal_train, petal_length_train)\n",
    "\n",
    "y_pred = model.predict(sepal_test)\n",
    "\n",
    "score = model.score(sepal_test, petal_length_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('MSE: ' + str(mean_squared_error(petal_length_test,y_pred)))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.9, 6.1, 4.7, 3.8, 4.9, 5.1, 4.5, 5. , 4.7, 5.2, 4.5, 1.6, 5.1,\n",
       "       4.2, 3.6, 4. , 4.6, 6. , 1.5, 1.1, 5.3, 4.2, 1.7, 1.5, 4.9, 1.5,\n",
       "       5.1, 3. , 1.4, 4.5, 6.1, 4.2, 1.4, 5.9, 5.7, 5.8, 5.6, 1.6, 1.6,\n",
       "       5.1, 5.7, 1.3, 5.4, 1.4, 5. , 5.4, 1.3, 1.4, 5.8, 1.4, 1.3, 1.7,\n",
       "       4. , 5.9, 6.6, 1.4, 1.5, 1.4, 4.5, 4.4, 1.2, 1.7, 4.3, 1.5, 6.9,\n",
       "       3.3, 6.4, 4.4, 1.5, 4.8, 1.2, 6.7, 1.5, 1.6, 6.1, 1.4, 5.6, 4.1,\n",
       "       3.9, 3.5, 5.3, 5.2, 4.9, 5. , 1.6, 3.7, 5.6, 5.1, 1.5, 4.6, 4.1,\n",
       "       4.8, 4.4, 1.3, 1.5, 1.5, 5.6, 4.1, 6.7, 1.4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, _ = load_iris(return_X_y=True)\n",
    "\n",
    "sepal = X[:, :2]\n",
    "petal_length = X[:, 2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sepal_train, sepal_test, petal_length_train, petal_length_test = train_test_split(sepal, petal_length, test_size=0.33, random_state=0)\n",
    "\n",
    "petal_length_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree regression\n",
    "\n",
    "Similar to decision tree classification, the decision regressor tree is composed of nodes, branches, and decision leaves.\n",
    "The decision regression trees aim to decrease different metrics while dealing with numeric data. However, they use the same algorithm as before. Decision trees are considered supervised learning methods because they predict values of responses by learning decision rules that are derived from features.\n",
    "\n",
    "Splitting the dataset on an attribute leads to standard deviation reduction. You will now read steps for calculating decision tree regression.\n",
    "Step 1\n",
    "Calculate the standard deviation of the target.\n",
    "Step 2\n",
    "Split the dataset on the different attributes. Calculate the standard deviation for each branch. Subtract the resulting standard deviation from the standard deviation before the split. The result is the standard deviation reduction.\n",
    "Step 3\n",
    "Choose the attribute with the largest standard deviation reduction for the decision node. This means the most homogeneity in the branch.\n",
    "Step 4\n",
    "Repeat steps 2 and 3 while calculating the coefficient of variation (CV).\n",
    "The stopping condition:\n",
    "Reaching the maximum count of iterations.\n",
    "The CV in each attribute has become less than some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.2946107784431136\n"
     ]
    }
   ],
   "source": [
    "### DT Regression, boston\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(criterion= 'mae', random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('MAE: ' + str(mean_absolute_error(y_test,y_pred)))\n",
    "\n",
    "# return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.4141514285714284\n"
     ]
    }
   ],
   "source": [
    "## RF Regression, IRIS\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X, _ = load_iris(return_X_y=True)\n",
    "\n",
    "sepal = X[:, :2]\n",
    "petal_length  = X[:, 2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sepal_train, sepal_test, petal_length_train, petal_length_test = train_test_split(sepal, petal_length, test_size=0.33, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(criterion=\"mse\", random_state=0, n_estimators = 25)\n",
    "\n",
    "model.fit(sepal_train, petal_length_train)\n",
    "\n",
    "y_pred = model.predict(sepal_test)\n",
    "\n",
    "score = model.score(sepal_test, petal_length_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('MAE: '+ str(mean_absolute_error(petal_length_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression\n",
    "\n",
    "The K-Nearest Neighbor (KNN) algorithm.\n",
    "The difference between KNN for solving classification and regression problems.\n",
    "Implementation of KNN for regression in Python.\n",
    "\n",
    "As discussed in previous lessons, the K-Nearest Neighbor classification is a non-parametric method which depends on the feature space to determine the output. A KNN model basically uses the k-closest data points to the test point to use as references for prediction.\n",
    "In classification, the output points to a class membership. A new point is classified based on the votes of the surrounding k-neighbors.\n",
    "Example:\n",
    "If k = 1, the class of a new data point is simply the same class as the nearest data point.\n",
    "However, what about regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5673733201680796\n"
     ]
    }
   ],
   "source": [
    "### Predicting housing prices using KNN\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0 )\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "print(score)\n",
    "\n",
    "# return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9271883296829704\n"
     ]
    }
   ],
   "source": [
    "### Predicting IRIS using KNN\n",
    "\n",
    "    from sklearn.datasets import load_iris\n",
    "    X, _ = load_iris(return_X_y=True)\n",
    "\n",
    "    sepal = X[:, :2]\n",
    "    petal_length  = X[:, 2]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    sepal_train, sepal_test, petal_length_train, petal_length_test = train_test_split(sepal, petal_length, test_size=0.33, random_state=0)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "    model.fit(sepal_train, petal_length_train)\n",
    "\n",
    "    y_pred = model.predict(sepal_test)\n",
    "\n",
    "    score = model.score(sepal_test, petal_length_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "114c2fdb226e4dc103f00c2512f291967d93737a0d1ce45be4aa564dac2afe8e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
